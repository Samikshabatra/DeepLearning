# DeepLearning Repository

This repository contains implementations of **Deep Learning laboratory experiments** completed as part of the academic curriculum.  
The experiments focus on understanding **neural network fundamentals**, **training mechanisms**, and **performance analysis** using modern deep learning frameworks.

---

## Objectives

- To understand the fundamentals of **Deep Learning and Neural Networks**
- To implement **Multi-Layer Perceptrons (MLP)** and **Deep Feedforward Neural Networks**
- To study the role of **activation functions, loss functions, and optimizers**
- To analyze the impact of **hyperparameters** such as learning rate, batch size, depth, and width
- To gain practical experience using **PyTorch**, **Keras** and **TensorFlow**

---

## Laboratory Experiments

###  Lab 1: Learning XOR Boolean Function using MLP
**Branch:** `Lab1`

**Description:**
- Implementation of a **Multi-Layer Perceptron (MLP)** to learn the XOR Boolean function
- Demonstrates why XOR is **not linearly separable**
- Highlights the importance of **hidden layers** and **nonlinear activation functions**
- Includes **hyperparameter tuning** and result analysis

**Files:**
- `MLP_XOR.ipynb`
- `README.md` (theory, objective, and explanation)

---

###  Lab 2: Deep Feedforward Neural Network for Fashion-MNIST Classification
**Branch:** `Lab2`

**Description:**
- Design and implementation of a **deep feedforward neural network**
- Classification of **Fashion-MNIST dataset**
- Experiments performed on:
  - Network depth and width
  - Different activation functions (ReLU, Sigmoid, Tanh, Leaky ReLU)
  - Training loss and test accuracy
- Visualization of **hidden layer activations**
- Performance comparison and analysis

---

## ðŸ›  Technologies Used

- Python
- PyTorch
- TensorFlow
- NumPy
- Pandas
- Matplotlib
- Jupyter Notebook / Google Colab

---

## Key Concepts Covered

- Feedforward Neural Networks
- Multi-Layer Perceptron (MLP)
- Backpropagation Algorithm
- Gradient Descent Optimization
- Cross-Entropy Loss
- Activation Functions
- Overfitting vs Generalization
- Hyperparameter Tuning
- Model Evaluation and Visualization

---

## Learning Outcomes

Through these laboratory experiments, I gained:
- Hands-on experience with deep learning model implementation
- A strong understanding of neural network training workflows
- The ability to analyze and compare model performance
- Practical exposure to PyTorch and TensorFlow frameworks

---

##  Author

**Samiksha Batra**  
Deep Learning   
Academic Project Repository

